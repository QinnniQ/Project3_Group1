{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project | Natural Language Processing\n",
    "### News Headline: Real or Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: xgboost in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: streamlit in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\happy\\anaconda3_32\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\happy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\happy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\happy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn nltk spacy xgboost matplotlib seaborn transformers datasets sentence-transformers streamlit\n",
    "from sympy import python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# python -m nltk.downloader stopwords punkt wordnet\n",
    "\n",
    "# python -m spacy download en_core_web_sm\n",
    "#%python -m spacy download en_core_web_sm\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# pip install pandas scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'dataset/predicted_validation_data_from_rev2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                              title  \\\n",
      "0      1  As U.S. budget fight looms, Republicans flip t...   \n",
      "1      1  U.S. military to accept transgender recruits o...   \n",
      "2      1  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3      1  FBI Russia probe helped by Australian diplomat...   \n",
      "4      1  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n",
      "4  December 29, 2017   \n",
      "label\n",
      "1    19999\n",
      "0    19943\n",
      "Name: count, dtype: int64\n",
      "label      0\n",
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Read training data\n",
    "df = pd.read_csv(r\"dataset\\data.csv\")\n",
    "# Reduce the training set to speed up development. \n",
    "print(df.head())\n",
    "print(df['label'].value_counts())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "subject",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f53630f4-3279-47e8-b55c-c76ffb17507c",
       "rows": [
        [
         "0",
         "2",
         "UK's May 'receiving regular updates' on London tube station incident: PM's office",
         "LONDON (Reuters) - British Prime Minister Theresa May is being regularly briefed after armed police rushed to an incident at a London underground station on Friday, with local media reporting there had been an explosion on a packed rush-hour commuter train.  The prime minister is receiving regular updates,  a spokesman from May s office said when asked if May was aware of the incident. ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "1",
         "2",
         "UK transport police leading investigation of London incident, counter-terrorism police aware",
         "LONDON (Reuters) - British counter-terrorism police were monitoring events after media reports of a blast on a train in Parsons Green station in west London on Friday, but the investigation into the incident is being led by transport police, a security source said. Armed police rushed to an incident at a London underground station on Friday with local media reporting there had been an explosion on a packed rush-hour commuter train. Counter-terrorism officers were on the scene of the incident.  ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "2",
         "2",
         "Pacific nations crack down on North Korean ships as Fiji probes more than 20 vessels",
         "WELLINGTON (Reuters) - South Pacific island nations are scouring shipping records for vessels with links to North Korea after Fiji said it had identified 20 falsely flagged ships it suspects the isolated regime is using to evade United Nations sanctions. Fiji, along with Interpol and the Singapore-based regional shipping regulator Tokyo MoU, are investigating the vessels for links to North Korea, a spokesman for the country s Maritime Safety Authority (MSAF) told Reuters on Friday. Interpol and Tokyo MoU did not immediately respond to requests for comment on their investigations. The 18 members of the Pacific Islands Forum this month agreed to launch an audit of every ship registered in the Pacific to search for any links to North Korea. New Zealand Foreign Affairs Minister Gerry Brownlee said Pacific countries, including his own, were concerned North Korea was using falsely flagged cargo ships as one avenue to trade goods in spite of sanctions.  What we do know is that declared North Korean GDP (gross domestic product) is not big enough to support the nuclear program that they re running so there has to be significant black market or offbook revenue going into the country,  Brownlee told Reuters in a phone interview this week. The move came as the UN on Monday ramped up sanctions on North Korea, including tightening up oversight of vessels on the high seas. Authorities will now be allowed to check suspected vessels for prohibited cargo with the authority of the flag country.  UN sanctions introduced in August banned North Korean exports of seafood as part of efforts to curtail the regime s access to foreign funds. North Korea had been expected to earn an estimated $295 million from seafood in 2017, one U.N. diplomat said. In Fiji s case, the North Korean-linked ships had adopted the island nation s flag without formally registering, the MSAF said. It was not known in what way the possibly more-than 20 vessels were linked to North Korea or what their suspected activities or locations were. The MSAF declined to provide details due to the ongoing investigation. In addition to the fraudulent use of flags, Pacific governments were concerned North Korean vessels could be quietly registering in nations that allow international ships to use their flags.     If it s not clear why they re on that register, in other words, they re not regular callers into Pacific ports, then further investigation is needed to untangle the ship owner,  said Brownlee. The review would take place over the next couple of months, Brownlee said, and Australia and New Zealand would provide intelligence to help the small island nations check any North Korean connections. ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "3",
         "2",
         "Three suspected al Qaeda militants killed in Yemen drone strike",
         "ADEN, Yemen (Reuters) - Three suspected al Qaeda militants were killed in a drone strike in southern Yemen late on Thursday, a local security official and residents said. The strike in Mudiyah district in Abyan province on the Arabian Sea coast had targeted a motorcycle which the suspected militants were riding, the official said. Abyan is one of several provinces in central and southern Yemen where Al Qaeda in the Arabian Peninsula (AQAP) and its local affiliate Ansar al-Sharia operate. AQAP has taken advantage of a more than two-year civil war between the Iran-aligned Houthi group and President Abd-Rabbu Mansour Hadi s Saudi-backed government to strengthen its position in the impoverished country. The United States has repeatedly attacked AQAP with aircraft and unmanned drones in what U.S. officials say is a campaign to wear down the group s ability to coordinate attacks abroad. The Saudi-led coalition supporting Hadi has also launched thousands of air strikes against the Houthis which control Yemen s capital and much of the country s north. The Houthis have in turn fired rockets towards Saudi cities and villages. They say their attacks are in response to Saudi strikes on Yemeni cities and villages. The coalition spokesman said on Thursday that a Saudi pilot had died a day earlier when his plane crashed in the southern province of Abyan due to  technical failure , the Saudi news agency SPA said. The war has killed more than 10,000 people. ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "4",
         "2",
         "Chinese academics prod Beijing to consider North Korea contingencies",
         "BEIJING (Reuters) - Chinese academics are publicly broaching the idea that China and the United States should share plans on how to deal with a  potential conflict on the Korean Peninsula, a sign some say of how North Korea s weapons test may be making Beijing more open to the once taboo subject. Chinese officials have rebuffed top U.S. military brass for years in their efforts to share contingencies for war or regime collapse in Pyongyang, suspicious over Washington s intentions and fearful that such engagement could further alienate North Korea, their once steadfast wartime ally. Jia Qingguo, the dean of the School of International Studies at China s elite Peking University and a respected advisor to Chinese leaders, said in an article this week that with the chances of war increasing daily, it might be time for China to  prepare for the worst .  When war becomes a real possibility, China must be prepared. And, with this in mind, China must be more willing to consider talks with concerned countries on contingency plans,  Jia wrote in an article first published on the online East Asia Forum run out of Australian National University. Given recent developments,  Beijing may have no better choice than to start talking with Washington and Seoul , Jia said. Ties between China and North Korea have deteriorated sharply in recent years, to the point some diplomats and experts fear Beijing may become, like Washington, a target of its neighbor s ire. Jia s comments, at the forefront of what is still a sensitive debate in China, were made before North Korea on Friday fired a missile that flew over Japan s northern Hokkaido far out into the Pacific Ocean.  That test came after the U.N. Security Council s latest round of sanctions on Monday banning North Korea s textile exports and capping fuel supplies in response to Pyongyang s accelerated effort to target the United States with a powerful, nuclear-tipped missile. Xie Tao, associate dean at the School of English and International Studies at Beijing Foreign Studies University, said Chinese academics in years past would typically only express their opinions on whether China should be sharing plans with Washington behind closed doors.  Now you can get all sorts of people from the left and right and they are allowed to make their views public,  Xie told Reuters.  You could interpret that as a signal that the Chinese government is perhaps willing now to consider, if not actually join up to, contingency plans,  Xie said.  My sense is that there must be some in-depth discussion about what could potentially happen if there is a collapse or internal chaos   how Beijing and Washington should respond to this.  During a trip to China in August, Joseph Dunford, Chairman of the U.S. Joint Chiefs of Staff, attended Chinese military drills at China s Northern Theater Command in the city of Shenyang, which covers China s border with North Korea.  Some analysts perceived that as China opening a crack in the door to its own preparations, but neither side are showing their cards to the public. Dunford said in Beijing during that visit that despite Washington s urgings the  conditions aren t set for such a detailed conversation  between the two militaries.  U.S. military officials have privately voiced frustration that previous channels established for direct military communications line are essentially unstaffed on the Chinese side. Dunford said creating a round-the-clock  responsive  communications link that could be used in a crisis was a top priority for military ties.  For a graphic on North Korea's missile and nuclear tests, click: here ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "5",
         "2",
         "Flames raced along train at west London station: eye witness",
         "LONDON (Reuters) - Flames engulfed one carriage and raced along a train on a west London route to Parsons Green, forcing passengers to trample others as they rushed for an exit, an eyewitness told Reuters. The man said people were trampled on when they fled the train after hearing a whoosh and seeing flames race towards them. He said he did not hear a bang after police rushed to an incident at the station at Parsons Green.  I just heard a kind of whoosh. I looked up and saw the whole carriage engulfed in flames making its way towards me,  he told Reuters, adding that the train was packed with people. ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "6",
         "2",
         "London police advise people to avoid area near station incident",
         "LONDON (Reuters) - British police on Friday advised people to avoid the area near Parsons Green, a west London underground station, after media reports of a blast on a train.  Officers from the Metropolitan Police Service and British Transport Police are in attendance along with the London Fire Brigade and London Ambulance Service,  London s police said in a statement.  We would advise people to avoid the area.   ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "7",
         "2",
         "London ambulance service sends hazardous area response team to station incident",
         "LONDON (Reuters) - London s ambulance service said on Friday it had sent its hazardous area response team to an incident at a west London metro station after reports of a blast.  We have sent multiple resources to the scene including single responders in cars, ambulance crews, incident response officers and our hazardous area response team,  the London Ambulance Service said on Twitter.  Our initial priority is to assess the level and nature of injuries.   London s transport authority said it had extended a suspension of the underground line which runs through Parsons Green metro station in west London. ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "8",
         "2",
         "Witness says injured in stampede at London station: Reuters reporter",
         "LONDON (Reuters) - A woman at London s Parsons Green underground train station told Reuters on Friday she was injured in a stampede.  Armed police were at the scene, a Reuters photographer said.  A blast on an underground train at Parsons Green left some passengers with facial burns at the station, London s Metro newspaper reported on its website. ",
         "worldnews",
         "September 15, 2017 "
        ],
        [
         "9",
         "2",
         "UK says world will stand together against North Korea after missile launch",
         "LONDON (Reuters) - Britain said on Friday the world would stand together against North Korea after the country fired a missile that flew over Japan into the Pacific Ocean, ratcheting up tensions after Pyongyang s test of a powerful nuclear bomb.  Yet another illegal missile launch by North Korea,  Foreign Secretary Boris Johnson tweeted.  UK and international community will stand together in the face of these provocations.  ",
         "worldnews",
         "September 15, 2017 "
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>UK's May 'receiving regular updates' on London...</td>\n",
       "      <td>LONDON (Reuters) - British Prime Minister Ther...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>UK transport police leading investigation of L...</td>\n",
       "      <td>LONDON (Reuters) - British counter-terrorism p...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pacific nations crack down on North Korean shi...</td>\n",
       "      <td>WELLINGTON (Reuters) - South Pacific island na...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Three suspected al Qaeda militants killed in Y...</td>\n",
       "      <td>ADEN, Yemen (Reuters) - Three suspected al Qae...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Chinese academics prod Beijing to consider Nor...</td>\n",
       "      <td>BEIJING (Reuters) - Chinese academics are publ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Flames raced along train at west London statio...</td>\n",
       "      <td>LONDON (Reuters) - Flames engulfed one carriag...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>London police advise people to avoid area near...</td>\n",
       "      <td>LONDON (Reuters) - British police on Friday ad...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>London ambulance service sends hazardous area ...</td>\n",
       "      <td>LONDON (Reuters) - London s ambulance service ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Witness says injured in stampede at London sta...</td>\n",
       "      <td>LONDON (Reuters) - A woman at London s Parsons...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>UK says world will stand together against Nort...</td>\n",
       "      <td>LONDON (Reuters) - Britain said on Friday the ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      2  UK's May 'receiving regular updates' on London...   \n",
       "1      2  UK transport police leading investigation of L...   \n",
       "2      2  Pacific nations crack down on North Korean shi...   \n",
       "3      2  Three suspected al Qaeda militants killed in Y...   \n",
       "4      2  Chinese academics prod Beijing to consider Nor...   \n",
       "5      2  Flames raced along train at west London statio...   \n",
       "6      2  London police advise people to avoid area near...   \n",
       "7      2  London ambulance service sends hazardous area ...   \n",
       "8      2  Witness says injured in stampede at London sta...   \n",
       "9      2  UK says world will stand together against Nort...   \n",
       "\n",
       "                                                text    subject  \\\n",
       "0  LONDON (Reuters) - British Prime Minister Ther...  worldnews   \n",
       "1  LONDON (Reuters) - British counter-terrorism p...  worldnews   \n",
       "2  WELLINGTON (Reuters) - South Pacific island na...  worldnews   \n",
       "3  ADEN, Yemen (Reuters) - Three suspected al Qae...  worldnews   \n",
       "4  BEIJING (Reuters) - Chinese academics are publ...  worldnews   \n",
       "5  LONDON (Reuters) - Flames engulfed one carriag...  worldnews   \n",
       "6  LONDON (Reuters) - British police on Friday ad...  worldnews   \n",
       "7  LONDON (Reuters) - London s ambulance service ...  worldnews   \n",
       "8  LONDON (Reuters) - A woman at London s Parsons...  worldnews   \n",
       "9  LONDON (Reuters) - Britain said on Friday the ...  worldnews   \n",
       "\n",
       "                  date  \n",
       "0  September 15, 2017   \n",
       "1  September 15, 2017   \n",
       "2  September 15, 2017   \n",
       "3  September 15, 2017   \n",
       "4  September 15, 2017   \n",
       "5  September 15, 2017   \n",
       "6  September 15, 2017   \n",
       "7  September 15, 2017   \n",
       "8  September 15, 2017   \n",
       "9  September 15, 2017   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read validation data\n",
    "val_data = pd.read_csv(r\"dataset\\validation_data.csv\")\n",
    "val_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Preprocess Text with spaCy + NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\happy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\"])\n",
    "\n",
    "def preprocess_text(text, remove_named_entities=True):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    doc = nlp(text)\n",
    "    ents = set(ent.text.lower() for ent in doc.ents) if remove_named_entities else set()\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.lemma_ not in stop_words\n",
    "        and token.is_alpha\n",
    "        and token.text.lower() not in ents\n",
    "    ]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m      6\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text, remove_named_entities)\u001b[0m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m---> 11\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[0;32m     12\u001b[0m ents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ent\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments) \u001b[38;5;28;01mif\u001b[39;00m remove_named_entities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     13\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_alpha\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ents\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\spacy\\ml\\tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m ParserStepModel(\n\u001b[0;32m     35\u001b[0m         X,\n\u001b[0;32m     36\u001b[0m         model\u001b[38;5;241m.\u001b[39mlayers,\n\u001b[0;32m     37\u001b[0m         unseen_classes\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munseen_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     38\u001b[0m         train\u001b[38;5;241m=\u001b[39mis_train,\n\u001b[0;32m     39\u001b[0m         has_upper\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_upper\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\spacy\\ml\\parser_model.pyx:250\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[1;34m(model, Xs, is_train)\u001b[0m\n\u001b[0;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[0;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[1;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m layer(Xf, is_train)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[0;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[1;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](X, is_train)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\happy\\anaconda3_32\\Lib\\site-packages\\thinc\\layers\\maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[1;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mgemm(X, W, trans2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[0;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Combine and clean\n",
    "train_df['content'] = train_df['title'] + \" \" + train_df['text']\n",
    "test_df['content'] = test_df['title'] + \" \" + test_df['text']\n",
    "\n",
    "train_df['clean_content'] = train_df['content'].apply(preprocess_text)\n",
    "test_df['clean_content'] = test_df['content'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_text = tfidf.fit_transform(train_df['clean_content'])\n",
    "X_test_text = tfidf.transform(test_df['clean_content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_df['subject_encoded'] = le.fit_transform(train_df['subject'])\n",
    "test_df['subject_encoded'] = le.transform(test_df['subject'])\n",
    "\n",
    "for df_ in [train_df, test_df]:\n",
    "    df_['year'] = df_['date'].dt.year\n",
    "    df_['month'] = df_['date'].dt.month\n",
    "    df_['weekday'] = df_['date'].dt.weekday\n",
    "\n",
    "X_train_meta = train_df[['subject_encoded', 'year', 'month', 'weekday']].values\n",
    "X_test_meta = test_df[['subject_encoded', 'year', 'month', 'weekday']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train = hstack([X_train_text, X_train_meta])\n",
    "X_test = hstack([X_test_text, X_test_meta])\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Full dataset (for CV later)\n",
    "X_all = hstack([tfidf.transform(df['title'] + \" \" + df['text'].apply(preprocess_text)), \n",
    "                df[['subject_encoded', 'year', 'month', 'weekday']].values])\n",
    "y_all = df['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_all, y_all, cv=skf, scoring='f1')\n",
    "\n",
    "print(\"5-Fold F1 Scores:\", cv_scores)\n",
    "print(\"Mean F1 Score:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"🔸 XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"🔸 XGBoost F1 Score:\", f1_score(y_test, y_pred_xgb))\n",
    "print(\"🔸 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "print(\"🔸 Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG Boost cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_xgb = cross_val_score(xgb_model, X_all, y_all, cv=skf, scoring='f1')\n",
    "print(\"🔸 XGBoost 5-Fold F1 Scores:\", cv_scores_xgb)\n",
    "print(\"🔸 Mean F1:\", cv_scores_xgb.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Create datasets\n",
    "bert_train_df = train_df[['clean_content', 'label']].rename(columns={'clean_content': 'text'})\n",
    "bert_test_df = test_df[['clean_content', 'label']].rename(columns={'clean_content': 'text'})\n",
    "\n",
    "train_ds = Dataset.from_pandas(bert_train_df)\n",
    "test_ds = Dataset.from_pandas(bert_test_df)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_function, batched=True)\n",
    "test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "predictions = trainer.predict(test_ds)\n",
    "y_pred_bert = np.argmax(predictions.predictions, axis=1)\n",
    "y_true_bert = predictions.label_ids\n",
    "\n",
    "print(\"🔷 BERT Accuracy:\", accuracy_score(y_true_bert, y_pred_bert))\n",
    "print(\"🔷 BERT F1 Score:\", f1_score(y_true_bert, y_pred_bert))\n",
    "print(\"🔷 BERT Classification Report:\\n\", classification_report(y_true_bert, y_pred_bert))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##OLD CODES for VAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_titles = val_data['title'].fillna(\"\")\n",
    "val_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform titles using the same vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tfidf = vectorizer.transform(val_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['label'] = predicted_labels\n",
    "val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the modified validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Predicted labels saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
